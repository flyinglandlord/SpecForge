{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1bcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from eagle.model.ea_model import EaModel\n",
    "from fastchat.model import get_conversation_template\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ec1b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0c9dd484e04eb1bf539b68daa81f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d2t': tensor([     0,      0,      0,  ..., 119648, 119669, 119669]), 'idk_index': tensor(32000), 'fc.weight': tensor([[-0.0703,  0.1602,  0.0400,  ...,  0.0061, -0.0762,  0.0057],\n",
      "        [-0.0200,  0.0718, -0.0206,  ..., -0.0269,  0.0596,  0.0217],\n",
      "        [-0.0272,  0.0645, -0.0427,  ..., -0.0732,  0.0135,  0.1069],\n",
      "        ...,\n",
      "        [ 0.0302, -0.0996,  0.1021,  ..., -0.0630, -0.0047, -0.0284],\n",
      "        [-0.0271, -0.0349, -0.0216,  ...,  0.0430, -0.0139,  0.0417],\n",
      "        [-0.0562,  0.0776,  0.0347,  ...,  0.0035,  0.0315, -0.0374]],\n",
      "       dtype=torch.bfloat16), 'lm_head.weight': tensor([[-0.0300, -0.0064, -0.0239,  ...,  0.0356, -0.0059, -0.0008],\n",
      "        [ 0.0272, -0.0025,  0.0032,  ...,  0.0192, -0.0209,  0.0195],\n",
      "        [ 0.0493,  0.0117,  0.0210,  ...,  0.0199,  0.0459,  0.0427],\n",
      "        ...,\n",
      "        [-0.0256,  0.0466, -0.0148,  ..., -0.0155,  0.0586,  0.0187],\n",
      "        [-0.0466,  0.0077,  0.0177,  ...,  0.0101,  0.0371, -0.0090],\n",
      "        [ 0.0013,  0.0225,  0.0103,  ..., -0.0175,  0.0229, -0.0173]],\n",
      "       dtype=torch.bfloat16), 'midlayer.hidden_norm.weight': tensor([0.8984, 0.9297, 0.9648,  ..., 0.9727, 0.9570, 0.9180],\n",
      "       dtype=torch.bfloat16), 'midlayer.input_layernorm.weight': tensor([1.0000, 0.9180, 0.9922,  ..., 0.9727, 0.9805, 0.9727],\n",
      "       dtype=torch.bfloat16), 'midlayer.mlp.down_proj.weight': tensor([[-0.0977,  0.1079,  0.0559,  ...,  0.0796,  0.1055, -0.0111],\n",
      "        [-0.0122,  0.0100, -0.0188,  ...,  0.0260, -0.0474, -0.0674],\n",
      "        [ 0.0223, -0.0840,  0.0361,  ..., -0.0129,  0.0007,  0.0216],\n",
      "        ...,\n",
      "        [ 0.0259,  0.0972,  0.0425,  ..., -0.0215, -0.0913, -0.0364],\n",
      "        [-0.0693, -0.0693,  0.0015,  ..., -0.0227, -0.0157,  0.0620],\n",
      "        [ 0.0208,  0.0081, -0.0077,  ...,  0.0098,  0.0640, -0.0371]],\n",
      "       dtype=torch.bfloat16), 'midlayer.mlp.gate_proj.weight': tensor([[ 0.0183,  0.0308,  0.0226,  ...,  0.0286, -0.0449, -0.0305],\n",
      "        [ 0.0791,  0.0664, -0.0352,  ..., -0.0108, -0.1001,  0.0298],\n",
      "        [-0.0228,  0.0420, -0.0557,  ...,  0.0074, -0.0635, -0.0053],\n",
      "        ...,\n",
      "        [-0.0118,  0.0280, -0.0239,  ..., -0.0398,  0.0137, -0.0464],\n",
      "        [-0.1064, -0.0155,  0.0253,  ...,  0.0005,  0.0767, -0.0762],\n",
      "        [ 0.0160,  0.0579, -0.0330,  ..., -0.0085,  0.0011, -0.0156]],\n",
      "       dtype=torch.bfloat16), 'midlayer.mlp.up_proj.weight': tensor([[-0.0210, -0.0918,  0.0060,  ..., -0.0200,  0.0033,  0.0060],\n",
      "        [ 0.0097, -0.0757, -0.0569,  ...,  0.0815,  0.0559, -0.0535],\n",
      "        [-0.1099,  0.0089, -0.0430,  ..., -0.0393,  0.0444, -0.0030],\n",
      "        ...,\n",
      "        [-0.1250,  0.0118, -0.0403,  ...,  0.0254, -0.0771, -0.0064],\n",
      "        [-0.1270, -0.0554,  0.0019,  ...,  0.0503,  0.0559, -0.0109],\n",
      "        [ 0.0605, -0.0195, -0.0098,  ...,  0.0005,  0.0903, -0.0109]],\n",
      "       dtype=torch.bfloat16), 'midlayer.post_attention_layernorm.weight': tensor([0.6484, 0.6484, 0.5977,  ..., 0.6484, 0.6992, 0.5547],\n",
      "       dtype=torch.bfloat16), 'midlayer.self_attn.k_proj.weight': tensor([[ 0.0786, -0.0273, -0.0737,  ..., -0.0079, -0.0082, -0.0327],\n",
      "        [ 0.0518,  0.0289,  0.0564,  ..., -0.0130, -0.0576, -0.0654],\n",
      "        [-0.0306,  0.0776,  0.0801,  ...,  0.0481, -0.0171,  0.0574],\n",
      "        ...,\n",
      "        [ 0.0045,  0.0166, -0.0557,  ...,  0.0454, -0.0713,  0.0400],\n",
      "        [ 0.0041,  0.0138,  0.0208,  ..., -0.0366, -0.0928, -0.0698],\n",
      "        [-0.0188,  0.0503, -0.0918,  ..., -0.0439, -0.0302, -0.0093]],\n",
      "       dtype=torch.bfloat16), 'midlayer.self_attn.o_proj.weight': tensor([[-0.0289,  0.0278,  0.0178,  ...,  0.0053,  0.0500, -0.0425],\n",
      "        [-0.0278, -0.0361, -0.0342,  ...,  0.0284, -0.0383,  0.0153],\n",
      "        [ 0.0085,  0.0635, -0.0058,  ...,  0.0330,  0.0088, -0.0371],\n",
      "        ...,\n",
      "        [-0.0303,  0.0398, -0.0752,  ..., -0.0381,  0.0400, -0.0854],\n",
      "        [ 0.0864, -0.0260,  0.0298,  ...,  0.0273,  0.0109,  0.0574],\n",
      "        [-0.0038,  0.0654,  0.0146,  ...,  0.0649, -0.0557, -0.0334]],\n",
      "       dtype=torch.bfloat16), 'midlayer.self_attn.q_proj.weight': tensor([[-0.0874,  0.0801, -0.0282,  ...,  0.0108, -0.0031,  0.0256],\n",
      "        [ 0.0162,  0.0237,  0.0544,  ..., -0.0065,  0.0055, -0.0227],\n",
      "        [-0.0889,  0.0272,  0.0145,  ...,  0.0515, -0.1211,  0.0403],\n",
      "        ...,\n",
      "        [ 0.0056, -0.0447, -0.0442,  ...,  0.0752, -0.0645,  0.0266],\n",
      "        [-0.0030, -0.0791, -0.0262,  ..., -0.0182, -0.0884, -0.0133],\n",
      "        [-0.0239,  0.0286,  0.0347,  ..., -0.0063,  0.0275, -0.0532]],\n",
      "       dtype=torch.bfloat16), 'midlayer.self_attn.v_proj.weight': tensor([[ 0.0815,  0.0211, -0.1152,  ..., -0.0289,  0.0684, -0.0186],\n",
      "        [ 0.0192, -0.0559,  0.0291,  ..., -0.0282, -0.0498,  0.0430],\n",
      "        [ 0.0262, -0.0226, -0.0625,  ..., -0.0261, -0.0767,  0.0050],\n",
      "        ...,\n",
      "        [-0.0018,  0.0349, -0.1641,  ..., -0.0322, -0.0217, -0.0938],\n",
      "        [-0.0415, -0.0376,  0.1001,  ..., -0.0200,  0.0219,  0.0481],\n",
      "        [ 0.0415, -0.1875,  0.0339,  ..., -0.0057, -0.0664,  0.0269]],\n",
      "       dtype=torch.bfloat16), 'norm.weight': tensor([1.2500, 1.3203, 1.3203,  ..., 1.1328, 1.2266, 1.3281],\n",
      "       dtype=torch.bfloat16), 't2d': tensor([ True,  True,  True,  ..., False, False, False])}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'d2t.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m base_model_path = \u001b[33m\"\u001b[39m\u001b[33m/mtc/models/qwen3-8b\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m EAGLE_model_path = \u001b[33m\"\u001b[39m\u001b[33m/mtc/chenjunyi1/project/SpecForge/outputs/qwen3-8b-eagle3-dynamic-sharegpt/epoch_9_step_603370\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mEaModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mea_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEAGLE_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m model.eval()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# base_model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#     base_model_path,\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#     torch_dtype=torch.float16,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mtc/chenjunyi1/project/ms-swift/hf_training_code/EAGLE/eagle/model/ea_model.py:140\u001b[39m, in \u001b[36mEaModel.from_pretrained\u001b[39m\u001b[34m(cls, use_eagle3, base_model_path, ea_model_path, total_token, depth, top_k, threshold, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mprint\u001b[39m(ea_layer_state_dict)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# fix the incompatible d2t and t2d keys when vocab size changes\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# ignore the addtional tokens for d2t\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m ea_layer_state_dict[\u001b[33m'\u001b[39m\u001b[33md2t.weight\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mea_layer_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43md2t.weight\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:\u001b[32m32000\u001b[39m]\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# ea_layer_state_dict['t2d.weight'] = ea_layer_state_dict['t2d.weight'][:base_model.config.vocab_size, :]\u001b[39;00m\n\u001b[32m    143\u001b[39m model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    144\u001b[39m     use_eagle3,\n\u001b[32m    145\u001b[39m     base_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m     ea_layer_state_dict\n\u001b[32m    153\u001b[39m )\n",
      "\u001b[31mKeyError\u001b[39m: 'd2t.weight'"
     ]
    }
   ],
   "source": [
    "# Load your model\n",
    "depth = 7\n",
    "base_model_path = \"/mtc/models/qwen3-8b\"\n",
    "EAGLE_model_path = \"/mtc/chenjunyi1/project/SpecForge/outputs/qwen3-8b-eagle3-dynamic-sharegpt/epoch_9_step_603370\"\n",
    "model = EaModel.from_pretrained(\n",
    "    base_model_path=base_model_path,\n",
    "    ea_model_path=EAGLE_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    total_token=depth+1,\n",
    "    depth=depth,\n",
    "    top_k=1,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_path,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ce3ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Complete the code I provided.from typing import List\n",
      "\n",
      "\n",
      "def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n",
      "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
      "    >>> filter_by_substring([], 'a')\n",
      "    []\n",
      "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
      "    ['abc', 'bacd', 'array']\n",
      "    \"\"\"\n",
      "\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Here's the completed code for your function:\n",
      "\n",
      "```python\n",
      "from typing import List\n",
      "\n",
      "def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n",
      "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
      "    >>> filter_by_substring([], 'a')\n",
      "    []\n",
      "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
      "    ['abc', 'bacd', 'array']\n",
      "    \"\"\"\n",
      "    return [s for s in strings if substring in s]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- The function uses a list comprehension to filter the input list `strings`.\n",
      "- It includes only those strings that contain the `substring` as a part of them.\n",
      "- The `in` operator checks if the substring exists within each string.\n",
      "\n",
      "This implementation is concise and efficient for the given task.\n",
      "281 177\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Complete the code I provided.from typing import List\\n\\n\\ndef filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    \\\"\\\"\\\" Filter an input list of strings only for ones that contain given substring\\n    >>> filter_by_substring([], 'a')\\n    []\\n    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\\n    ['abc', 'bacd', 'array']\\n    \\\"\\\"\\\"\\n\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "# Note: this draft model is used for thinking mode disabled\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(base_model.device)\n",
    "\n",
    "input_ids = model_inputs[\"input_ids\"]\n",
    "prompt_len = input_ids.shape[1]\n",
    "\n",
    "max_new_tokens = 2048\n",
    "\n",
    "generated_ids = input_ids.clone()\n",
    "top1_probs = []     # p(x_t | x_<t>)\n",
    "tokens = []\n",
    "total_steps = 0\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    out = base_model(input_ids=generated_ids)\n",
    "    logits = out.logits[:, -1, :]           # [1, vocab]\n",
    "    next_token = logits.argmax(dim=-1)      # [1, ]\n",
    "    top1_probs.append(F.softmax(logits, dim=-1)[0, next_token].item())\n",
    "    tokens.append(next_token.item())\n",
    "\n",
    "    generated_ids = torch.cat(\n",
    "        [generated_ids, next_token[:, None]], dim=1\n",
    "    )\n",
    "    total_steps += 1\n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "print(len(generated_ids[0]), total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c75408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from transformers import PreTrainedModel, PretrainedConfig, AutoConfig\n",
    "\n",
    "from eagle.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM\n",
    "from eagle.model.modeling_mixtral_kv import MixtralForCausalLM as KVMixtralForCausalLM\n",
    "#from .modeling_qwen2_kv import LlamaForCausalLM as KVQwen2ForCausalLM\n",
    "from eagle.model.modeling_qwen2_kv import Qwen2ForCausalLM as KVQwen2ForCausalLM\n",
    "from eagle.model.modeling_qwen3_kv import Qwen3ForCausalLM as KVQwen3ForCausalLM\n",
    "from eagle.model.utils import *\n",
    "from eagle.model.kv_cache import initialize_past_key_values\n",
    "\n",
    "from eagle.model.cnets import Model\n",
    "from eagle.model.cnets1 import Model as Model1\n",
    "from eagle.model.configs import EConfig\n",
    "\n",
    "def reject_all_exit_hook(hook_state):\n",
    "    return 0\n",
    "\n",
    "def new_evaluate_posterior(\n",
    "        logits: torch.Tensor,\n",
    "        candidates: torch.Tensor,\n",
    "        logits_processor,\n",
    "):\n",
    "    accept_length = 0\n",
    "    best_candidate = torch.tensor(0, dtype=torch.long, device=candidates.device)\n",
    "    return best_candidate, accept_length, logits[best_candidate, accept_length]\n",
    "\n",
    "def new_eagenerate_with_early_exit_hook(\n",
    "    self,\n",
    "    input_ids,\n",
    "    temperature=0.0,\n",
    "    top_p=0.0,\n",
    "    top_k=1,            # by default, use greedy sampling for draft model, and only verify draft token chain\n",
    "    max_new_tokens=512,\n",
    "    max_length=2048,\n",
    "    log=False,\n",
    "    is_llama3=False,\n",
    "    early_exit_hook=None,  # Hook function to predict accept length for early exit\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate tokens with early exit hook for MTP module evaluation.\n",
    "    \n",
    "    Args:\n",
    "        early_exit_hook: A callable that takes (logits, candidates, hidden_state_new, outputs, step) \n",
    "                        and returns predicted accept length for early exit simulation.\n",
    "    \n",
    "    Returns:\n",
    "        If log=False: input_ids\n",
    "        If log=True: (input_ids, stats_dict) where stats_dict contains:\n",
    "            - total_steps: total number of decoding steps\n",
    "            - total_tokens: total number of generated tokens\n",
    "            - true_accept_lengths: list of true accept lengths at each step\n",
    "            - predicted_accept_lengths: list of predicted accept lengths at each step\n",
    "            - early_exit_triggered: list of bools indicating if early exit was triggered\n",
    "            - true_acceptance_rate: average true acceptance rate\n",
    "            - predicted_acceptance_rate: average predicted acceptance rate\n",
    "            - acceptance_rate_gap: difference between predicted and true rates\n",
    "            - avg_true_accept_length: average true accept length\n",
    "            - avg_predicted_accept_length: average predicted accept length\n",
    "    \"\"\"\n",
    "    if is_llama3:\n",
    "        stop_token_id = self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "    if temperature > 1e-5:\n",
    "        logits_processor = prepare_logits_processor(temperature=temperature, top_p=top_p, top_k=top_k)\n",
    "    else:\n",
    "        logits_processor = None\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        'true_accept_lengths': [],\n",
    "        'predicted_accept_lengths': [],\n",
    "        'early_exit_triggered': [],\n",
    "        'total_steps': 0,\n",
    "        'total_tokens': 0,\n",
    "    }\n",
    "\n",
    "    padding = (torch.zeros(1, 1, dtype=torch.long) - 1).to(input_ids.device)\n",
    "    input_ids = input_ids.clone()\n",
    "    self.ea_layer.reset_kv()\n",
    "\n",
    "    # Initialize the past key and value states\n",
    "    if hasattr(self, \"past_key_values\"):\n",
    "        past_key_values = self.past_key_values\n",
    "        past_key_values_data = self.past_key_values_data\n",
    "        current_length_data = self.current_length_data\n",
    "        # Reset the past key and value states\n",
    "        current_length_data.zero_()\n",
    "    else:\n",
    "        (\n",
    "            past_key_values,\n",
    "            past_key_values_data,\n",
    "            current_length_data,\n",
    "        ) = initialize_past_key_values(self.base_model, max_length=max_length)\n",
    "        self.past_key_values = past_key_values\n",
    "        self.past_key_values_data = past_key_values_data\n",
    "        self.current_length_data = current_length_data\n",
    "\n",
    "    input_len = input_ids.shape[1]\n",
    "    reset_tree_mode(self)\n",
    "    # prefill\n",
    "    draft_tokens, retrieve_indices, tree_mask, tree_position_ids, logits, hidden_state, sample_token, mtp_logits = initialize_tree(\n",
    "        input_ids, self, past_key_values, logits_processor\n",
    "    )\n",
    "    new_token = 0\n",
    "    max_length = max_length - self.ea_layer.total_tokens - 10\n",
    "    sample_p = logits[-1].softmax(dim=-1)\n",
    "\n",
    "    for idx in range(max_length):\n",
    "        self.base_model.model.tree_mask = tree_mask\n",
    "\n",
    "        draft_tokens = draft_tokens.to(input_ids.device)\n",
    "        # Target model forward, get logits\n",
    "        logits, hidden_state_new, outputs = tree_decoding(\n",
    "            self,\n",
    "            draft_tokens,\n",
    "            past_key_values,\n",
    "            tree_position_ids,\n",
    "            input_ids,\n",
    "            retrieve_indices,\n",
    "        )\n",
    "        \n",
    "        draft_tokens = torch.cat((draft_tokens, padding), dim=1)\n",
    "        candidates = draft_tokens[0, retrieve_indices]\n",
    "        \n",
    "        # Apply early exit hook BEFORE evaluate_posterior to predict this round's accept length\n",
    "        # using state information from tree_decoding (logits, hidden_state_new, outputs)\n",
    "        predicted_accept_length = 0\n",
    "        early_exit = False\n",
    "        \n",
    "        if early_exit_hook is not None:\n",
    "            # try:\n",
    "                # Hook gets state information BEFORE verification and predicts accept length\n",
    "            hook_state = {\n",
    "                'mtp_module_logits': mtp_logits,\n",
    "                'draft_candidates': candidates,\n",
    "                'main_model_logits': logits,\n",
    "                'main_model_hidden_state': hidden_state_new,\n",
    "                'main_model_outputs': outputs,\n",
    "                'step': idx,\n",
    "                'input_ids': input_ids,\n",
    "                'retrieve_indices': retrieve_indices,\n",
    "                'p_token': sample_p,\n",
    "            }\n",
    "            predicted_accept_length = early_exit_hook(hook_state)\n",
    "            \n",
    "            # Ensure predicted length is valid\n",
    "            if isinstance(predicted_accept_length, torch.Tensor):\n",
    "                predicted_accept_length = predicted_accept_length.item()\n",
    "            predicted_accept_length = max(0, min(predicted_accept_length, candidates.shape[1] - 1))\n",
    "                \n",
    "            # except Exception as e:\n",
    "            #     print(f\"Warning: early_exit_hook failed at step {idx}: {e}\")\n",
    "            #     print(hook_state['mtp_module_logits'].shape, \n",
    "            #         hook_state['draft_candidates'].shape, \n",
    "            #         hook_state['mtp_module_hidden_state'].shape,\n",
    "            #         hook_state['retrieve_indices'],\n",
    "            #         hook_state['p_token'].shape,\n",
    "            #         hook_state['input_ids'].shape)\n",
    "            #     predicted_accept_length = 0\n",
    "        \n",
    "        stats['predicted_accept_lengths'].append(predicted_accept_length)\n",
    "        \n",
    "        # Verification - get true accept length\n",
    "        _, true_accept_length, _ = evaluate_posterior(\n",
    "            logits, candidates, logits_processor\n",
    "        )\n",
    "        best_candidate, accept_length, sample_p = new_evaluate_posterior(\n",
    "            logits, candidates, logits_processor\n",
    "        )\n",
    "        \n",
    "        # Record true accept length\n",
    "        # true_accept_length = accept_length.item() if isinstance(accept_length, torch.Tensor) else accept_length\n",
    "        stats['true_accept_lengths'].append(true_accept_length)\n",
    "        \n",
    "        # Check if early exit would trigger (predicted < true)\n",
    "        if predicted_accept_length < true_accept_length:\n",
    "            early_exit = True\n",
    "        \n",
    "        stats['early_exit_triggered'].append(early_exit)\n",
    "        \n",
    "        # Use true accept length for actual generation (to maintain correctness)\n",
    "        # but record the difference for evaluation\n",
    "        before_input = input_ids.clone()\n",
    "        input_ids, draft_tokens, retrieve_indices, tree_mask, tree_position_ids, new_token, hidden_state, sample_token, mtp_logits = update_inference_inputs(\n",
    "            input_ids,\n",
    "            candidates,\n",
    "            best_candidate,\n",
    "            predicted_accept_length,\n",
    "            retrieve_indices,\n",
    "            logits_processor,\n",
    "            new_token,\n",
    "            past_key_values_data,\n",
    "            current_length_data,\n",
    "            self,\n",
    "            hidden_state_new,\n",
    "            sample_p\n",
    "        )\n",
    "        after_input = input_ids.clone()\n",
    "        \n",
    "        stats['total_steps'] += 1\n",
    "\n",
    "        if is_llama3:\n",
    "            if stop_token_id in input_ids[0, input_len:].tolist():\n",
    "                break\n",
    "\n",
    "        if self.tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "        if new_token > max_new_tokens:\n",
    "            break\n",
    "        if input_ids.shape[1] > max_length:\n",
    "            break\n",
    "    \n",
    "    stats['total_tokens'] = new_token\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    if stats['total_steps'] > 0:\n",
    "        stats['avg_true_accept_length'] = sum(stats['true_accept_lengths']) / len(stats['true_accept_lengths'])\n",
    "        stats['avg_predicted_accept_length'] = sum(stats['predicted_accept_lengths']) / len(stats['predicted_accept_lengths'])\n",
    "        \n",
    "        # Acceptance rate: (total accepted tokens) / (total candidates)\n",
    "        max_candidates = candidates.shape[1] - 1 if 'candidates' in locals() else self.ea_layer.total_tokens\n",
    "        stats['true_acceptance_rate'] = stats['avg_true_accept_length'] / max_candidates if max_candidates > 0 else 0\n",
    "        stats['predicted_acceptance_rate'] = stats['avg_predicted_accept_length'] / max_candidates if max_candidates > 0 else 0\n",
    "        stats['acceptance_rate_gap'] = abs(stats['predicted_acceptance_rate'] - stats['true_acceptance_rate'])\n",
    "        \n",
    "        # Early exit statistics\n",
    "        stats['early_exit_rate'] = sum(stats['early_exit_triggered']) / len(stats['early_exit_triggered'])\n",
    "        stats['avg_length_difference'] = sum([t - p for t, p in zip(stats['true_accept_lengths'], stats['predicted_accept_lengths'])]) / len(stats['true_accept_lengths'])\n",
    "    else:\n",
    "        stats['avg_true_accept_length'] = 0\n",
    "        stats['avg_predicted_accept_length'] = 0\n",
    "        stats['true_acceptance_rate'] = 0\n",
    "        stats['predicted_acceptance_rate'] = 0\n",
    "        stats['acceptance_rate_gap'] = 0\n",
    "        stats['early_exit_rate'] = 0\n",
    "        stats['avg_length_difference'] = 0\n",
    "    \n",
    "    if not log:\n",
    "        return input_ids\n",
    "    else:\n",
    "        return input_ids, stats\n",
    "\n",
    "EaModel.eagenerate_with_early_exit_hook = new_eagenerate_with_early_exit_hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32841b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Complete the code I provided.from typing import List\n",
      "\n",
      "\n",
      "def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n",
      "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
      "    >>> filter_by_substring([], 'a')\n",
      "    []\n",
      "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
      "    ['abc', 'bacd', 'array']\n",
      "    \"\"\"\n",
      "\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Here's the completed code for your function:\n",
      "\n",
      "```python\n",
      "from typing import List\n",
      "\n",
      "def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n",
      "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
      "    >>> filter_by_substring([], 'a')\n",
      "    []\n",
      "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
      "    ['abc', 'bacd', 'array']\n",
      "    \"\"\"\n",
      "    return [s for s in strings if substring in s]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- The function uses a list comprehension to filter the input list `strings`.\n",
      "- It includes only those strings that contain the `substring` as a part of them.\n",
      "- The `in` operator checks if the substring exists within each string.\n",
      "\n",
      "This implementation is concise and efficient for the given task.\n",
      "177 281\n"
     ]
    }
   ],
   "source": [
    "# Generate with early exit hook and collect statistics\n",
    "output_ids, stats = model.eagenerate_with_early_exit_hook(\n",
    "    input_ids=input_ids,\n",
    "    log=True,  # Important: set log=True to get statistics\n",
    "    max_new_tokens=1024,\n",
    "    top_k=1,\n",
    "    early_exit_hook=reject_all_exit_hook,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "print(stats['total_steps'], len(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39022f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaccepted token avg top1 prob: 0.8999036153157552\n",
      "Accepted tokens avg top1 prob for next accept_length tokens: tensor(0.7574, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# in each step, stats the average of top1 prob of next \"accepted token length\" tokens\n",
    "neg_avg_top1_prob = 0.0\n",
    "neg_cnt = 0\n",
    "acc_avg_top1_prob = 0.0\n",
    "acc_cnt = 0\n",
    "for step in range(len(top1_probs)):\n",
    "    accept_length = stats['true_accept_lengths'][step] + 1\n",
    "    if accept_length == 1:\n",
    "        # Find the next place where accept_length > 0\n",
    "        for look_ahead in range(step+1, len(top1_probs)):\n",
    "            if stats['true_accept_lengths'][look_ahead] > 0:\n",
    "                accept_length = stats['true_accept_lengths'][look_ahead] + 1\n",
    "                break\n",
    "        neg_avg_top1_prob += sum(top1_probs[step:look_ahead])\n",
    "        neg_cnt += look_ahead - step\n",
    "    else:\n",
    "        acc_avg_top1_prob += sum(top1_probs[step:step+accept_length])\n",
    "        acc_cnt += accept_length + 1\n",
    "\n",
    "print(\"Unaccepted token avg top1 prob:\", neg_avg_top1_prob / neg_cnt)\n",
    "print(\"Accepted tokens avg top1 prob for next accept_length tokens:\", acc_avg_top1_prob / acc_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b9e2e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Method 1: Greedy True Accept ==========\n",
      "Average accept length: 1.3733\n",
      "Steps: 75\n",
      "\n",
      "========== Method 2: Oracle Top1 Prob ==========\n",
      "Average accept length: 0.9667\n",
      "Steps: 90\n",
      "\n",
      "========== Method 3: Optimal DP (Upper Bound) ==========\n",
      "Optimal average accept length (lambda*): 1.3600\n",
      "Recovered path average: 1.3600\n",
      "Steps: 75\n",
      "\n",
      "========== Summary ==========\n",
      "Greedy true        : 1.3733\n",
      "Oracle top1 prob   : 0.9667\n",
      "DP optimal (upper) : 1.3600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Utils\n",
    "# ============================================================\n",
    "\n",
    "def greedy_true_accept(true_accept_length):\n",
    "    \"\"\"\n",
    "    方法一：greedy 使用 true_accept_length\n",
    "    \"\"\"\n",
    "    N = len(true_accept_length)\n",
    "    i = 0\n",
    "    accepts = []\n",
    "    path = []\n",
    "\n",
    "    while i < N:\n",
    "        L = int(true_accept_length[i])\n",
    "        accepts.append(L)\n",
    "        path.append((i, L))\n",
    "        i = i + L + 1\n",
    "\n",
    "    return np.mean(accepts), accepts, path\n",
    "\n",
    "\n",
    "def oracle_top1_accept(true_accept_length, top1_prob, prob_threshold=0.9):\n",
    "    \"\"\"\n",
    "    方法二：全知 oracle，根据连续 top1 prob 决定接受长度\n",
    "    约束：L <= true_accept_length[i]\n",
    "    策略：连续 token 的 min prob >= threshold\n",
    "    \"\"\"\n",
    "    N = len(true_accept_length)\n",
    "    i = 0\n",
    "    accepts = []\n",
    "    path = []\n",
    "\n",
    "    while i < N:\n",
    "        max_L = int(true_accept_length[i])\n",
    "        L = 0\n",
    "        for l in range(1, max_L + 1):\n",
    "            if i + l < N and np.min(top1_prob[i+1:i+l+1]) >= prob_threshold:\n",
    "                L = l\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        accepts.append(L)\n",
    "        path.append((i, L))\n",
    "        i = i + L + 1\n",
    "\n",
    "    return np.mean(accepts), accepts, path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DP: Check if average >= lambda\n",
    "# ============================================================\n",
    "\n",
    "def dp_check_lambda(true_accept_length, lam):\n",
    "    \"\"\"\n",
    "    判定是否存在一条路径，使得 average_accept_length >= lam\n",
    "    返回 dp[0]\n",
    "    \"\"\"\n",
    "    N = len(true_accept_length)\n",
    "    dp = np.full(N + 1, -np.inf)\n",
    "    dp[N] = 0.0\n",
    "\n",
    "    for i in range(N - 1, -1, -1):\n",
    "        best = -np.inf\n",
    "        max_L = int(true_accept_length[i])\n",
    "        for L in range(max_L + 1):\n",
    "            j = i + L + 1\n",
    "            if j <= N:\n",
    "                val = (L - lam) + dp[j]\n",
    "                best = max(best, val)\n",
    "        dp[i] = best\n",
    "\n",
    "    return dp[0]\n",
    "\n",
    "\n",
    "def find_max_average_accept(true_accept_length, eps=1e-4):\n",
    "    \"\"\"\n",
    "    二分搜索最大平均接受长度 lambda\n",
    "    \"\"\"\n",
    "    lo, hi = 0.0, np.max(true_accept_length)\n",
    "\n",
    "    while hi - lo > eps:\n",
    "        mid = (lo + hi) / 2\n",
    "        if dp_check_lambda(true_accept_length, mid) >= 0:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "\n",
    "    return lo\n",
    "\n",
    "\n",
    "def recover_optimal_path(true_accept_length, lam):\n",
    "    \"\"\"\n",
    "    在给定最优 lambda 下恢复最优路径\n",
    "    \"\"\"\n",
    "    N = len(true_accept_length)\n",
    "    dp = np.full(N + 1, -np.inf)\n",
    "    choice = np.zeros(N, dtype=int)\n",
    "    dp[N] = 0.0\n",
    "\n",
    "    for i in range(N - 1, -1, -1):\n",
    "        best = -np.inf\n",
    "        best_L = 0\n",
    "        max_L = int(true_accept_length[i])\n",
    "        for L in range(max_L + 1):\n",
    "            j = i + L + 1\n",
    "            if j <= N:\n",
    "                val = (L - lam) + dp[j]\n",
    "                if val > best:\n",
    "                    best = val\n",
    "                    best_L = L\n",
    "        dp[i] = best\n",
    "        choice[i] = best_L\n",
    "\n",
    "    # rollout\n",
    "    i = 0\n",
    "    accepts = []\n",
    "    path = []\n",
    "    while i < N:\n",
    "        L = choice[i]\n",
    "        accepts.append(L)\n",
    "        path.append((i, L))\n",
    "        i = i + L + 1\n",
    "\n",
    "    return np.mean(accepts), accepts, path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Example Test\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 模拟 true_accept_length（例如 MTP 验证结果）\n",
    "    true_accept_length = np.array([x.cpu().numpy() for x in stats['true_accept_lengths']])\n",
    "\n",
    "    # 模拟 top1_prob（带相关性）\n",
    "    top1_prob = np.array(top1_probs)\n",
    "\n",
    "    N = len(true_accept_length)\n",
    "\n",
    "    print(\"========== Method 1: Greedy True Accept ==========\")\n",
    "    avg1, acc1, path1 = greedy_true_accept(true_accept_length)\n",
    "    print(f\"Average accept length: {avg1:.4f}\")\n",
    "    print(f\"Steps: {len(acc1)}\")\n",
    "\n",
    "    print(\"\\n========== Method 2: Oracle Top1 Prob ==========\")\n",
    "    avg2, acc2, path2 = oracle_top1_accept(\n",
    "        true_accept_length,\n",
    "        top1_prob,\n",
    "        prob_threshold=0.9\n",
    "    )\n",
    "    print(f\"Average accept length: {avg2:.4f}\")\n",
    "    print(f\"Steps: {len(acc2)}\")\n",
    "\n",
    "    print(\"\\n========== Method 3: Optimal DP (Upper Bound) ==========\")\n",
    "    lam_star = find_max_average_accept(true_accept_length)\n",
    "    avg3, acc3, path3 = recover_optimal_path(true_accept_length, lam_star)\n",
    "\n",
    "    print(f\"Optimal average accept length (lambda*): {lam_star:.4f}\")\n",
    "    print(f\"Recovered path average: {avg3:.4f}\")\n",
    "    print(f\"Steps: {len(acc3)}\")\n",
    "\n",
    "    print(\"\\n========== Summary ==========\")\n",
    "    print(f\"Greedy true        : {avg1:.4f}\")\n",
    "    print(f\"Oracle top1 prob   : {avg2:.4f}\")\n",
    "    print(f\"DP optimal (upper) : {avg3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bba4eca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全局最优平均接受长度: 1.3600\n",
      "[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 1), (10, 0), (11, 1), (13, 4), (18, 3), (22, 2), (25, 0), (26, 2), (29, 3), (33, 0), (34, 0), (35, 1), (37, 6), (44, 7), (52, 4), (57, 2), (60, 3), (64, 4), (69, 3), (73, 2), (76, 3), (80, 2), (83, 3), (87, 3), (91, 2), (94, 4), (99, 1), (101, 2), (104, 0), (105, 2), (108, 0), (109, 0), (110, 3), (114, 3), (118, 0), (119, 1), (121, 1), (123, 0), (124, 1), (126, 0), (127, 0), (128, 1), (130, 4), (135, 0), (136, 2), (139, 0), (140, 0), (141, 1), (143, 1), (145, 0), (146, 1), (148, 4), (153, 0), (154, 2), (157, 1), (159, 0), (160, 0), (161, 0), (162, 0), (163, 0), (164, 1), (166, 1), (168, 2), (171, 0), (172, 1), (174, 1), (176, 0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_true_optimal_average(true_accept_length):\n",
    "    n = len(true_accept_length)\n",
    "    # dist[i] 表示到达位置 i 时能获得的最大 (总长度 - rho * 总步数)\n",
    "    # 这是一种 Dinkelbach 算法的思想，但为了直观，我们直接用搜索或改版的 DP\n",
    "    \n",
    "    # 既然我们要找的是最大平均值，我们可以尝试所有可能的步数\n",
    "    # dp[i][s] 表示到达位置 i，恰好用了 s 步时的最大总接受长度\n",
    "    dp = np.full((n + 1, n + 1), -1.0)\n",
    "    parent = {} # 用于回溯路径: (i, s) -> (prev_i, prev_L)\n",
    "\n",
    "    dp[0][0] = 0\n",
    "    \n",
    "    for s in range(n): # 步数\n",
    "        for i in range(n): # 当前位置\n",
    "            if dp[i][s] == -1: continue\n",
    "            \n",
    "            max_L = int(true_accept_length[i])\n",
    "            # 尝试所有可能的接受长度 L (从 0 到 max_L)\n",
    "            for L in range(max_L + 1):\n",
    "                next_pos = i + L + 1\n",
    "                if next_pos > n:\n",
    "                    actual_L = n - i - 1\n",
    "                    target_pos = n\n",
    "                else:\n",
    "                    actual_L = L\n",
    "                    target_pos = next_pos\n",
    "                \n",
    "                new_len = dp[i][s] + actual_L\n",
    "                if new_len > dp[target_pos][s+1]:\n",
    "                    dp[target_pos][s+1] = new_len\n",
    "                    parent[(target_pos, s+1)] = (i, actual_L)\n",
    "\n",
    "    # 寻找全局最大平均值\n",
    "    best_avg = -1\n",
    "    best_config = (0, 0) # (pos, steps)\n",
    "\n",
    "    for s in range(1, n + 1):\n",
    "        avg = dp[n][s] / s\n",
    "        if avg > best_avg:\n",
    "            best_avg = avg\n",
    "            best_config = (n, s)\n",
    "\n",
    "    # 回溯路径\n",
    "    path = []\n",
    "    curr_pos, curr_s = best_config\n",
    "    while curr_s > 0:\n",
    "        prev_pos, L = parent[(curr_pos, curr_s)]\n",
    "        path.append((prev_pos, L))\n",
    "        curr_pos = prev_pos\n",
    "        curr_s -= 1\n",
    "    \n",
    "    return best_avg, path[::-1]\n",
    "\n",
    "# 测试逻辑\n",
    "# true_accept_length = [0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0] # 故意在位置1放个大的\n",
    "# avg, path = find_true_optimal_average(true_accept_length)\n",
    "# 示例数据\n",
    "true_accept_length = stats['true_accept_lengths']\n",
    "avg, p = find_global_optimal_path(true_accept_length)\n",
    "print(f\"全局最优平均接受长度: {avg:.4f}\")\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
